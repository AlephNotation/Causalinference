

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[12pt]{article}


\usepackage{amsmath, amsthm, amssymb, setspace, fullpage, enumitem, listings}
\usepackage{apacite}
\usepackage[margin=0.7in]{geometry}
\usepackage[english]{babel}


\renewcommand{\qedsymbol}{$\scriptstyle \blacksquare$}
\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}
\newcommand{\dto}{\overset{d}{\to}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\F}{\mathfrak{F}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\diag}{\mathrm{diag}}
\renewcommand{\P}{\mathrm{P}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\betav}{\vec{\beta}}
\newcommand{\betahat}{\hat{\vec{\beta}}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\plim}{\operatornamewithlimits{plim}}
\newcommand{\interior}{\operatornamewithlimits{int}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
\copy0\kern-\wd0\mkern4mu\box0}}  % statistical independence symbol

\newtheorem{thm}{Theorem}[section]
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{axiom}[thm]{Axiom}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}

\theoremstyle{definition}
\newtheorem{example}[thm]{Example}

\theoremstyle{definition}
\newtheorem{assumption}[thm]{Assumption}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}


\onehalfspace
\setlength{\parskip}{1ex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Causal Inference in Python: A Vignette}
\author{Laurence Wong}
\maketitle

In this document we illustrate the use of CausalInference with a simple simulated data set.

\section{Initialization} \label{sec.a}

The main object of interest in CausalInference is the class \texttt{CausalModel}, which we can import with
\begin{verbatim}
  >>> from causalinference import CausalModel
\end{verbatim}
\texttt{CausalModel} takes as inputs three NumPy arrays: \texttt{Y}, an $N$-vector of observed outcomes; \texttt{D}, an $N$-vector of treatment status indicators; and \texttt{X}, an $N$-by-$K$ matrix of covariates. To initialize a \texttt{CausalModel} instance, simply run:
\begin{verbatim}
  >>> causal = CausalModel(Y, D, X)
\end{verbatim}

Once an instance of the class \texttt{CausalModel} has been created, it will contain a number of attributes and methods that are relevant for conducting a causal analysis. Tables \ref{tab.a} and \ref{tab.b} contain a brief description of these attributes and methods.

\texttt{CausalModel} is \textit{stateful}. As we employ some of the methods to be discussed subsequently, the instance \texttt{causal} will mutate, with new data being added or existing data being modified or dropped. Running
\begin{verbatim}
  >> causal.reset()
\end{verbatim}
will return \texttt{causal} to its initial state.

\section{Summary Statistics} \label{sec.b}

Once an instance of the class \texttt{CausalModel} has been created, basic summary statistics will be computed and stored in the attribute \texttt{summary\_stats}. We can display it by running:
\begin{verbatim}
  >>> print causal.summary_stats
  Summary Statistics
  
                         Controls (N_c=392)         Treated (N_t=608)             
         Variable         Mean         S.d.         Mean         S.d.     Raw-diff
  --------------------------------------------------------------------------------
                Y       43.097       31.353       90.911       41.815       47.814
  
                         Controls (N_c=392)         Treated (N_t=608)             
         Variable         Mean         S.d.         Mean         S.d.     Nor-diff
  --------------------------------------------------------------------------------
               X0        3.810        2.950        5.762        2.566        0.706
               X1        3.436        2.848        5.849        2.634        0.880
\end{verbatim}

The attribute \texttt{summary\_stats} is in reality just a dictionary-like object with special method defined to enable the display of the above table. In many situations it is more convenient to simply access the relevant statistic directly. To retrieve the vector of covariate mean for the treatment group, for example, we simply run:
\begin{verbatim}
  >>> causal.summary_stats['X_t_mean']
  array([ 5.76232357,  5.8489734 ])
\end{verbatim}

Since \texttt{summary\_stats} behaves like a dictionary, it is equipped with the usual Python dictionary methods. To list the dictionary keys, for instance, we go:
\begin{verbatim}
  >>> causal.summary_stats.keys()
  ['Y_c_mean', 'X_t_sd', 'N_t', 'K', 'ndiff', 'N', 'Y_t_sd', 'rdiff', 'Y_t_mean',
  'X_c_mean', 'X_t_mean', 'Y_c_sd', 'X_c_sd', 'N_c']
\end{verbatim}

Most of the statistics appearing in the summary table should be self-explanatory, with the possible exception of the normalized differences in average covariates. This statistic is defined as
\[\frac{\bar{X}_{k,t} - \bar{X}_{k,c}}{\sqrt{\left(s^2_{k,t}+s^2_{k,c}\right)\Big/ 2}},\]
where $\bar{X}_{k,t}$ and $s_{k,t}$ are the sample mean and sample standard deviation of the $k$th covariate of the treatment group, and $\bar{X}_{k,c}$ and $s_{k,c}$ are the analogous statistics for the control group.

The normalized differences in average covariates provide a way to measure the covariate balance between the treatment and the control groups. Unlike the t-statistic, its absolute magnitude does not increase (in expectation) as the sample size increases.

\section{Propensity Score Estimation} \label{sec.c}

The propensity score, defined as the probability of getting treatment conditional on the covariates, plays a central role in much of what follows. Two methods, \texttt{est\_propensity} and \texttt{est\_propensity\_s}, are provided for propensity score estimation. Both involve running a logistic regression of the treatment indicator $D$ on functions of the covariates. \texttt{est\_propensity} allows the user to specify the covariates to include linearly and/or quadratically, while \texttt{est\_propensity\_s} will make this choice automatically based on a sequence of likelihood ratio tests.

In the following, we run \texttt{est\_propensity\_s} and display the estimation results. In this example, the specification selection algorithm decided to include both covariates and all the interaction and quadratic terms.

\begin{verbatim}
  >>> causal.est_propensity_s()
  >>> print causal.propensity
  Estimated Parameters of Propensity Score
  
                      Coef.       S.e.          z      P>|z|      [95% Conf. int.]
  --------------------------------------------------------------------------------
       Intercept     -2.839      0.526     -5.401      0.000     -3.870     -1.809
              X1      0.486      0.153      3.178      0.001      0.186      0.786
              X0      0.466      0.155      3.011      0.003      0.163      0.770
           X1*X0      0.080      0.015      5.391      0.000      0.051      0.109
           X0*X0     -0.045      0.012     -3.579      0.000     -0.069     -0.020
           X1*X1     -0.045      0.013     -3.542      0.000     -0.070     -0.020
\end{verbatim}

Like \texttt{summary\_stats}, the \texttt{propensity} attribule is in reality another dictionary-like container of results. The dictionary keys of \texttt{propensity} can be found by running:
\begin{verbatim}
  >>> causal.propensity.keys()
  ['coef', 'lin', 'qua', 'loglike', 'fitted', 'se']
\end{verbatim}
The estimated propensity scores can be recovered by accessing \texttt{causal.propensity['fitted']}. Though we won't make direct calls to it, most of the propensity-based techniques discussed subsequently are based on this vector.

\section{Improving Covariate Balance} \label{sec.d}

When there is indication of covariate imbalance, we may wish to construct a sample where the treatment and control groups are more similar than the original full sample. One way of doing so is by dropping units with extreme values of propensity score. For these subjects, their covariate values are such that the probability of being in the treatment (or control) group is so overwhelmingly high that we cannot reliably find comparable units in the opposite group. We may wish to forego estimating treatment effects for such units since nothing much can be credibly said about them.

A good rule-of-thumb is to drop units whose estimated propensity score is less than $\alpha=0.1$ or greater than $1-\alpha=0.9$. By default, once the propensity score has been estimated by running either \texttt{est\_propensity} or \texttt{est\_propensity\_s}, a value of 0.1 will be set for the attribute \texttt{cutoff}:

\begin{verbatim}
  >>> causal.cutoff
  0.1
\end{verbatim}

Calling \texttt{causal.trim()} at this point will drop every unit that has propensity score outside of the $[\alpha, 1-\alpha]$ interval. Alternatively, a procedure exists that will estimate the optimal cutoff. The method \texttt{trim\_s} will perform this calculation, set the \texttt{cutoff} to the optimal $\alpha$, and then invoke \texttt{trim} to construct the subsample. For our example, the optimal $\alpha$ was estimated to be slightly less than 0.1:
\begin{verbatim}
  >>> causal.trim_s()
  >>> causal.cutoff
  0.0954928016329
\end{verbatim}

If we now print \texttt{summary\_stats} again to view the summary statistics of the trimmed sample, we see that the normalized differences in average covariates has fallen noticeably.
\begin{verbatim}
  >>> print causal.summary_stats
  Summary Statistics
  
                         Controls (N_c=371)         Treated (N_t=363)             
         Variable         Mean         S.d.         Mean         S.d.     Raw-diff
  --------------------------------------------------------------------------------
                Y       41.331       29.608       66.067       28.108       24.736
  
                         Controls (N_c=371)         Treated (N_t=363)             
         Variable         Mean         S.d.         Mean         S.d.     Nor-diff
  --------------------------------------------------------------------------------
               X0        3.709        2.872        4.658        2.522        0.351
               X1        3.407        2.784        4.661        2.517        0.472
\end{verbatim}

\section{Stratifying the Sample} \label{sec.e}

With the propensity score estimated, one may wish to stratify the sample into blocks that have units that are more similar in terms of their covariates. This makes the treatment and control groups within each propensity bin more comparable, and therefore treatment effect estimates more credible.

CausalInference provides two methods for subclassification based on propensity score. The first, \texttt{stratify}, splits the sample based on what is specified in the attribute \texttt{blocks}. The default value of \texttt{blocks} is set to 5, which means that \texttt{stratify} will split the sample into 5 equal-sized bins. In contrast, the second method, \texttt{stratify\_s}, will use a data-driven procedure for selecting both the number of blocks and their boundaries, with the expectation that the number of blocks should increase with the sample size.

To inspect the results of the stratification, we can invoke \texttt{print} on the attribute \texttt{strata} to display some summary statistics, as follows:
\begin{verbatim}
  >>> causal.stratify_s()
  >>> print causal.strata
  Stratification Summary
  
                Propensity Score         Sample Size     Ave. Propensity   Outcome
     Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff
  --------------------------------------------------------------------------------
           1     0.095     0.265       157        28     0.188     0.187    11.885
           2     0.266     0.474       111        72     0.360     0.367    12.025
           3     0.477     0.728        70       113     0.598     0.601    11.696
           4     0.728     0.836        23        69     0.781     0.787    10.510
           5     0.838     0.904        10        81     0.865     0.873     3.405
\end{verbatim}

Under the hood, the attribute \texttt{strata} is actually a list-like object that contains, as each of its elements, a full instance of the class \texttt{CausalModel}, with the input data being those that correspond to the units that are in the propensity bin. We can thus, for example, access each stratum and inspect its \texttt{summary\_stats} attribute, or as the following illustrates, loop through \texttt{strata} and estimate within-bin treatment effects using least squares.
\begin{verbatim}
  >>> for stratum in causal.strata:
  ...     stratum.est_via_ols(adj=1)
  ... 
  >>> [stratum.estimates['ols']['ate'] for stratum in causal.strata]
  [10.379170390195197, 9.2918973715823707, 9.67876709257445, 9.6722830043583023,
  9.2239596078238222]
\end{verbatim}

Note these estimates are much more stable and closer to the true value of 10 than the within-bin raw differences in average outcomes that were reported in the stratification summary table, highlighting the virtue of further controlling for covariates even within blocks.

Instead of manually looping through the \texttt{strata} attribute, estimating within-bin treatment effects, and then averaging appropriately to arrive at an overall estimate, we can also simply call \texttt{est\_via\_blocking}. We will report the resulting estimates in the next section along with estimates obtained from other, alternative estimators.

\section{Treatment Effect Estimation} \label{sec.f}

In addition to least squares and the blocking estimator described in the last section, CausalInference provides two alternative treatment effect estimators. The first is the nearest neighborhood matching estimator of Abadie and Imbens \citeyear{AbadieImbens.2006}. Instead of relying on the propensity score, this estimator constructs matching treatment and control units by matching directly on the covariate vectors themselves. The method \texttt{est\_via\_matching} implements this estimator, and provides several optional arguments that can be used to specify certain key parameters.

The last estimator is a version of the Horvitz-Thompson estimator, modified to further adjust for covariates. Mechanically, this involves running the following weight least squares regression:
\[Y_i = \alpha + \tau D_i + X_i \beta + \varepsilon_i,\]
where the weight for unit $i$ is $1/\hat{p}(X)$ if $i$ is in the treatment group, and $1/\big(1-\hat{p}(X)\big)$ if $i$ is in the control group. This estimator is also sometimes called the doubly-robust estimator, referring to the fact that this estimator is consistent if either the specification of the propensity score is correct, or the specification of the regression function is correct. We can invoke it by calling \texttt{est\_via\_weighting}.

In the following we invoke each of the four estimators (including least squares, since the input data has changed now that the sample has been trimmed), and print out the resulting estimates.
\begin{verbatim}
  >>> causal.est_via_ols()
  >>> causal.est_via_weighting()
  >>> causal.est_via_blocking()
  >>> causal.est_via_matching(bias_adj=True)
  Treatment Effect Estimates: Weighting
  
                       Est.       S.e.          z      P>|z|      [95% Conf. int.]
  --------------------------------------------------------------------------------
             ATE     17.821      1.684     10.585      0.000     14.521     21.121
  
  Treatment Effect Estimates: OLS
  
                       Est.       S.e.          z      P>|z|      [95% Conf. int.]
  --------------------------------------------------------------------------------
             ATE      2.913      0.803      3.627      0.000      1.339      4.487
             ATC      2.435      0.824      2.956      0.003      0.820      4.049
             ATT      3.401      0.885      3.843      0.000      1.667      5.136
  
  Treatment Effect Estimates: Blocking
  
                       Est.       S.e.          z      P>|z|      [95% Conf. int.]
  --------------------------------------------------------------------------------
             ATE      9.702      0.381     25.444      0.000      8.954     10.449
             ATC      9.847      0.527     18.701      0.000      8.815     10.879
             ATT      9.553      0.332     28.771      0.000      8.903     10.204
  
  Treatment Effect Estimates: Matching
  
                       Est.       S.e.          z      P>|z|      [95% Conf. int.]
  --------------------------------------------------------------------------------
             ATE      9.624      0.245     39.354      0.000      9.145     10.103
             ATC      9.642      0.270     35.776      0.000      9.114     10.170
             ATT      9.606      0.318     30.159      0.000      8.981     10.230
\end{verbatim}

\section{Simulated Data} \label{sec.g}

The data generating process that generated the example data set was chosen so that:
\begin{itemize}
\item[i.)] Unconfoundedness, $\big(Y(0), Y(1)\big) \independent D \, | \, X$, is satisfied.
\item[ii.)] Unconditionally, selection is not random. In particular, the probability of being assigned treatment is a function of the covariates $X$.
\item[iii.)] Potential outcomes $Y(0)$ and $Y(1)$ are nonlinear functions of $X$ plus random error.
\end{itemize}

Property (i) ensures that the tools available in CausalInference are actually appropriate. Properties (ii) and (iii) were adopted to illustrate how standard linear methods can fail even if selection is only on observables. A detailed description of the data simulation can be found in the Appendix.

\begin{table}[h]
\begin{center}\begin{tabular}{ll}
Attribute & Description \\
\texttt{summary\_stats} & Dictionary-like object containing summary statistics for the \\
& covariate variables. \\
\texttt{propensity} & Dictionary-like object containing propensity score data, \\
& including estimated logistic regression coefficients, predicted \\
& propensity score, maximized log-likelihood, and the lists of the \\
& linear and quadratic terms that are included in the regression. \\
\texttt{cutoff} & Floating point number specifying the cutoff point for trimming \\
& on propensity score.\\
\texttt{blocks} & Either an integer indicating the number of equal-sized blocks to \\
& stratify the sample into, or a list of ascending numbers specifying \\
& the boundaries of each stratum. \\
\texttt{strata} & List-like object containing the list of stratified propensity bins. \\
\texttt{estimates} & Dictionary-like object containing treatment effect estimates for \\
& each estimator used.
\end{tabular}\end{center}
\caption{Attributes of the class \texttt{CausalModel}. Invoking \texttt{print} on any of the dictionary- or list-like attribute above yields customized summary tables. Note that some attributes are only created after the relevant methods have been called.}  \label{tab.a}
\end{table}


\begin{table}[h]
\begin{center}\begin{tabular}{ll}
Method & Description \\
\texttt{reset} & Reinitializes data to original inputs, and drop any \\
& estimated results. \\
\texttt{est\_propensity} & Estimates via logistic regression the propensity score using specified \\
& linear and quadratic terms. \\
\texttt{est\_propensity\_s} & Estimates via logistic regression the propensity score using the \\
& covariate selection algorithm of Imbens and Rubin \citeyear{ImbensRubin.2015}. \\
\texttt{trim} & Trims data based on propensity score using the threshold \\
& specified by the attribute \texttt{cutoff}. \\
\texttt{trim\_s} & Trims data based on propensity score using the cutoff \\
& selected by the procedure of Crump, Hotz, Imbens, \\
& and Mitnik \citeyear{CrumpHotzImbensMitnik.2009}. \\
\texttt{stratify} & Stratifies the sample based on propensity score as \\
& specified by the attribute \texttt{blocks}. \\
\texttt{stratify\_s} & Stratifies the sample based on propensity score \\
& using the bin selection procedure suggested by \\
& Imbens and Rubin \citeyear{ImbensRubin.2015}. \\
\texttt{est\_via\_blocking} & Estimates average treatment effects using regression \\
& within blocks. \\
\texttt{est\_via\_matching} & Estimates average treatment effects using matching \\
& with replacement. \\
\texttt{est\_via\_weighting} & Estimates average treatment effects using the \\
& Horvitz-Thompson weighting estimator modified to \\
& incorporate covariates. \\
\texttt{est\_via\_ols} & Estimates average treatment effects using least squares.
\end{tabular}\end{center}
\caption{Methods of the class \texttt{CausalModel}. Invoke \texttt{help} on any of the above methods for more detailed documentation.}  \label{tab.b}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apacite}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

