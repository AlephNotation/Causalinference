
import numpy as np
import random
import itertools
import statsmodels.api as sm
from scipy.stats import norm


class CausalModel(object):

	def __init__(self, Y, D, X):
		self.Y, self.D, self.X = Y, D, X
		self.N, self.k = self.X.shape
		control, treated = (self.D==0), (self.D==1)
		self.Y_c, self.Y_t = self.Y[control], self.Y[treated]
		self.X_c, self.X_t = self.X[control], self.X[treated]
		self.N_c, self.N_t = self.X_c.shape[0], self.X_t.shape[0]

	def synthetic(self, poly=0):

		ITT = np.zeros(self.N_t)

		if poly > 1:

			terms = []
			for power in xrange(2, poly+1):
				terms.extend(list(itertools.combinations_with_replacement(range(self.k), power)))
			num_of_terms = len(terms)

			X_poly = np.ones((self.N, num_of_terms))
			for i in xrange(num_of_terms):
				for j in terms[i]:
					X_poly[:, i] = X_poly[:, i] * X[:, j]
			X_control = np.hstack((self.X_c, X_poly[D==0]))
			X_treated = np.hstack((self.X_t, X_poly[D==1]))

			for i in xrange(self.N_t):
				w = np.linalg.lstsq(X_control.T, X_treated[i, ])[0]
				ITT[i] = self.Y_t[i] - np.dot(w, self.Y_c)

		else:
			# avoids storing the potentially massive N_t-by-N_c weights matrix
			for i in xrange(self.N_t):
				w = np.linalg.lstsq(self.X_c.T, self.X_t[i, ])[0]
				ITT[i] = self.Y_t[i] - np.dot(w, self.Y_c)

		return Results(self, ITT.mean(), ITT)

	def matching(self, replace=False, correct_bias=False):

		match_index = np.zeros(self.N_t, dtype=np.int)

		if replace:
			for i in xrange(self.N_t):
				dX = self.X_c - self.X_t[i]
				match_index[i] = np.argmin((dX**2).sum(axis=1))
		else:
			unmatched = range(self.N_c)
			for i in xrange(self.N_t):
				dX = self.X_c[unmatched] - self.X_t[i]
				match_index[i] = unmatched.pop(np.argmin((dX**2).sum(axis=1)))

		if correct_bias:
			reg = sm.OLS(self.Y_c[match_index],
			             sm.add_constant(self.X_c[match_index])).fit()
			ITT = (self.Y_t - self.Y_c[match_index] - 
			       np.dot((self.X_t - self.X_c[match_index]), reg.params[1:]))
		else:
			ITT = self.Y_t - self.Y_c[match_index]

		return Results(self, ITT.mean(), ITT)

	def ols(self):

		D = self.D.reshape((self.N, 1))  # convert D into N-by-1 vector
		dX = self.X - self.X.mean(0)  # demean covariates
		DdX = D * dX
		# construct design matrix
		Z = np.column_stack((D, dX, DdX))

		reg = sm.OLS(self.Y, sm.add_constant(Z)).fit()

		# for derivation of this estimator, see my notes on Treatment Effects
		ITT = reg.params[1] + np.dot(dX[self.D==1], reg.params[-self.k:])

		return Results(self, ITT.mean(), ITT)


class Results(object):

	def __init__(self, model, ATT, ITT):
		self.ATT = ATT
		self.ITT = ITT

	def summary(self):

		print 'Estimated Average Treatment Effect on the Treated:', self.ATT









class parameters(object):

	"""
	Class object that stores the parameter values for use in the baseline model.
	
	See SimulateData function for model description.

	Args:
		N = Sample size (control + treated units) to generate
		k = Number of covariates
	"""

	def __init__(self, N=500, k=3):  # set initial parameter values
		self.N = N  # sample size (control + treated units)
		self.k = k  # number of covariates

		self.delta = 3
		self.beta = np.ones(k)
		self.theta = np.ones(k)
		self.mu = np.zeros(k)
		self.Sigma = np.identity(k)
		self.Gamma = np.identity(2)


def SimulateData(para=parameters(), nonlinear=False, return_counterfactual=False):

	"""
	Function that generates data according to one of two simple models that
	satisfies the Unconfoundedness assumption.

	The covariates and error terms are generated according to
		X ~ N(mu, Sigma), epsilon ~ N(0, Gamma).
	The counterfactual outcomes are generated by
		Y_0 = X*beta + epsilon_0,
		Y_1 = delta + X*(beta+theta) + epsilon_1,
	if the nonlinear Boolean is False, or by
		Y_0 = sum_of_abs(X) + epsilon_0,
		Y_1 = sum_of_squares(X) + epsilon_1,
	if the nonlinear Boolean is True.

	Selection is done according to the following propensity score function:
		P(D=1|X) = Phi(X*beta),
	where Phi is the standard normal CDF.

	Args:
		para = Model parameter values supplied by parameter class object
		nonlinear = Boolean indicating whether the data generating model should
		            be linear or not
		return_counterfactual = Boolean indicating whether to return vectors of
		                        counterfactual outcomes

	Returns:
		Y = N-dimensional array of observed outcomes
		D = N-dimensional array of treatment indicator; 1=treated, 0=control
		X = N-by-k matrix of covariates
		Y_0 = N-dimensional array of non-treated outcomes
		Y_1 = N-dimensional array of treated outcomes
	"""

	k = len(para.mu)

	X = np.random.multivariate_normal(mean=para.mu, cov=para.Sigma,
	                                  size=para.N)
	epsilon = np.random.multivariate_normal(mean=np.zeros(2), cov=para.Gamma,
	                                        size=para.N)

	Xbeta = np.dot(X, para.beta)

	pscore = norm.cdf(Xbeta)
	# for each p in pscore, generate Bernoulli rv with success probability p
	D = np.array([np.random.binomial(1, p, size=1) for p in pscore]).flatten()

	if nonlinear:
		Y_0 = abs(X).sum(1) + epsilon[:, 0]
		Y_1 = (X**2).sum(1) + epsilon[:, 1]
	else:
		Y_0 = Xbeta + epsilon[:, 0]
		Y_1 = para.delta + np.dot(X, para.beta+para.theta) + epsilon[:, 1]

	Y = (1 - D) * Y_0 + D * Y_1  # compute observed outcome

	if return_counterfactual:
		return Y, D, X, Y_0, Y_1
	else:
		return Y, D, X
