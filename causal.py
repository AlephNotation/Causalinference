
import numpy as np
import random
import statsmodels.api as sm
from scipy.stats import norm

class CausalModel(object):

	def __init__(self, Y, D, X):
		self.Y = Y
		self.D = D
		self.X = X
		self.N = self.X.shape[0]
		self.k = self.X.shape[1]
		control = (self.D==0)
		treated = (self.D==1)
		self.Y_c = self.Y[control]
		self.Y_t = self.Y[treated]
		self.X_c = self.X[control]
		self.X_t = self.X[treated]
		self.N_c = self.X_c.shape[0]
		self.N_t = self.X_t.shape[0]

	def synthetic(self):

		ITT = np.zeros(self.N_t)

		# avoids storing the potentially massive N_t-by-N_c weights matrix
		for i in xrange(self.N_t):
			w = np.linalg.lstsq(self.X_c.T, self.X_t[i, ])[0]
			ITT[i] = self.Y_t[i] - np.dot(w, self.Y_c)

		return Results(self, ITT.mean(), ITT)

	def matching(self):

		unmatched = range(self.N_c)
		match_index = np.zeros(self.N_t, dtype=np.int)

		for i in xrange(self.N_t):
			dX = self.X_c[unmatched] - self.X_t[i]
			j = np.argmin((dX**2).sum(axis=1))
			match_index[i] = unmatched[j]
			unmatched = np.delete(unmatched, j)

		ITT = self.Y_t - self.Y_c[match_index]

		return Results(self, ITT.mean(), ITT)

	def ols(self):

		D = self.D.reshape((self.N, 1))  # convert D into N-by-1 vector
		dX = self.X - self.X.mean(0)  # demean covariates
		DdX = D * dX
		# construct design matrix
		Z = np.column_stack((D, dX, DdX))

		reg = sm.OLS(self.Y, sm.add_constant(Z)).fit()

		# for derivation of this estimator, see my notes on Treatment Effects
		ITT = reg.params[1] + np.dot(dX[self.D==1], reg.params[-self.k:])

		return Results(self, ITT.mean(), ITT)


class Results(object):

	def __init__(self, model, ATT, ITT):
		self.ATT = ATT
		self.ITT = ITT

	def summary(self):

		print 'Estimated Average Treatment Effect on the Treated:', self.ATT









class parameters(object):

	"""
	Class object that stores the parameter values for use in the baseline model.
	
	See SimulateData function for model description.

	Args:
		N = Sample size (control + treated units) to generate
		k = Number of covariates
	"""

	def __init__(self, N=500, k=3):  # set initial parameter values
		self.N = N  # sample size (control + treated units)
		self.k = k  # number of covariates

		self.delta = 3
		self.beta = np.ones(k)
		self.theta = np.ones(k)
		self.mu = np.zeros(k)
		self.Sigma = np.identity(k)
		self.Gamma = np.identity(2)


def SimulateData(para=parameters(), nonlinear=False, return_counterfactual=False):

	"""
	Function that generates data according to one of two simple models that
	satisfies the Unconfoundedness assumption.

	The covariates and error terms are generated according to
		X ~ N(mu, Sigma), epsilon ~ N(0, Gamma).
	The counterfactual outcomes are generated by
		Y_0 = X*beta + epsilon_0,
		Y_1 = delta + X*(beta+theta) + epsilon_1,
	if the nonlinear Boolean is False, or by
		Y_0 = sum_of_abs(X) + epsilon_0,
		Y_1 = sum_of_squares(X) + epsilon_1,
	if the nonlinear Boolean is True.

	Selection is done according to the following propensity score function:
		P(D=1|X) = Phi(X*beta),
	where Phi is the standard normal CDF.

	Args:
		para = Model parameter values supplied by parameter class object
		nonlinear = Boolean indicating whether the data generating model should
		            be linear or not
		return_counterfactual = Boolean indicating whether to return vectors of
		                        counterfactual outcomes

	Returns:
		Y = N-dimensional array of observed outcomes
		D = N-dimensional array of treatment indicator; 1=treated, 0=control
		X = N-by-k matrix of covariates
		Y_0 = N-dimensional array of non-treated outcomes
		Y_1 = N-dimensional array of treated outcomes
	"""

	k = len(para.mu)

	X = np.random.multivariate_normal(mean=para.mu, cov=para.Sigma,
	                                  size=para.N)
	epsilon = np.random.multivariate_normal(mean=np.zeros(2), cov=para.Gamma,
	                                        size=para.N)

	Xbeta = np.dot(X, para.beta)

	pscore = norm.cdf(Xbeta)
	# for each p in pscore, generate Bernoulli rv with success probability p
	D = np.array([np.random.binomial(1, p, size=1) for p in pscore]).flatten()

	if nonlinear:
		Y_0 = abs(X).sum(1) + epsilon[:, 0]
		Y_1 = (X**2).sum(1) + epsilon[:, 1]
	else:
		Y_0 = Xbeta + epsilon[:, 0]
		Y_1 = para.delta + np.dot(X, para.beta+para.theta) + epsilon[:, 1]

	Y = (1 - D) * Y_0 + D * Y_1  # compute observed outcome

	if return_counterfactual:
		return Y, D, X, Y_0, Y_1
	else:
		return Y, D, X
